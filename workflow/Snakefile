###############################################################################
#  S n a k e m a k e   w o r k f l o w
###############################################################################


###############################################################################
# 0. Global convenience
###############################################################################
import glob, json, os, pathlib, multiprocessing as mp, psutil, re
ENV_YAML = os.path.join(os.getcwd(), "env", "environment.yml")

configfile: "workflow/config.yaml"
DATASETS     = config["datasets"]
SIG_SIZES    = config["sig_sizes"]
FIXED_K      = config["fixed_k"]
RUN_CORES    = str(config.get("run_cores", "NA"))

###############################################################################
# Phase A – executed **inside Docker** one run per core count
###############################################################################
rule all:
    """
    Main stage-1 build (plus stage-2 fixed-K).
    We conditionally require wall_clock_<cores>.tsv
    ONLY if run_cores != "NA".
    """
    input:
        # stage-1 summary tsvs
        expand("results/{ds}/stage1/summary_stage1.tsv",ds=DATASETS),
        expand("results/{ds}/stage1/system_info.json",ds=DATASETS),

        *(
            expand(f"results/{{ds}}/stage1/wall_clock_{RUN_CORES}.tsv",ds=DATASETS)
            if RUN_CORES != "NA"
            else []
        ),

        # per-K figures
        expand("figures/{ds}/stage1_k{K}/",ds=DATASETS,K=SIG_SIZES),

        # metric overview PNGs
        expand("figures/{ds}/stage1_summary/{f}.png",ds=DATASETS,
            f=["MCE", "Sensitivity", "Specificity"]),

        # Stage-2 fixed-K evaluation
        expand("results/{ds}/stage2/metrics.tsv",ds=DATASETS)

###############################################################################
# Stage 0 – copy raw matrix
###############################################################################
rule copy_matrix:
    input: "data/raw/{ds}.csv"
    output: "data/processed/{ds}_matrix.csv"
    conda: ENV_YAML
    shell: "cp {input} {output}"

###############################################################################
#  *hardware-fingerprint*  one JSON per dataset × run
###############################################################################
rule stage1_sysinfo:
    output: "results/{ds}/stage1/system_info.json"
    run:
        import platform, time

        out = pathlib.Path(output[0]);
        out.parent.mkdir(parents=True,exist_ok=True)
        info = {
            "timestamp": time.strftime("%Y-%m-%dT%H:%M:%S"),
            "python_version": platform.python_version(),
            "interpreter": platform.python_implementation(),
            "os": platform.platform(),
            "logical_cores": mp.cpu_count(),
            "total_ram_GiB": round(psutil.virtual_memory().total / 2 ** 30,1),
            "run_cores": RUN_CORES
        }
        out.write_text(json.dumps(info,indent=2))
        print(f"[sysinfo] wrote → {out}")

###############################################################################
# Stage 1 – Monte-Carlo CV for every K
###############################################################################
rule mccv_stage1:
    input:
        matrix="data/processed/{ds}_matrix.csv"
    output:
        metrics="results/{ds}/stage1/metrics_k{K}.tsv",
        freq="results/{ds}/stage1/freq_k{K}.csv"
    params:
        k="{K}",
        n_splits=config["n_splits"]
    benchmark:
        "results/{ds}/stage1/wall_clock_k{K}.txt"
    conda: ENV_YAML
    script: "../scripts/python/mccv_stage1.py"

###############################################################################
# 1️_one critical-path wall time per *run*
###############################################################################
OUT_TEMPLATE = f"results/{{ds}}/stage1/wall_clock_{RUN_CORES}.tsv"

rule record_wall_clock:
    """
    Aggregates the benchmark files from all K,
    and writes one critical-path time: max of all K's benchmark times
    """
    input:
        expand("results/{ds}/stage1/wall_clock_k{K}.txt",
            ds=DATASETS,K=SIG_SIZES)
    output: OUT_TEMPLATE
    run:
        import pathlib

        cores = int(config.get("run_cores",os.environ.get("SNAKEMAKE_NJOBS","-1")))
        times = []
        for fp in input:
            with open(fp) as fh:

                header = next(fh,None)
                line2 = next(fh,None)
                if line2:
                    v_str = line2.split("\t")[0].strip()
                    try:
                        times.append(float(v_str))
                    except ValueError:
                        pass
        if not times:
            raise ValueError("no valid wall_clock_k*.txt data found")
        wall_clock = max(times)
        out = pathlib.Path(output[0])
        out.parent.mkdir(parents=True,exist_ok=True)
        out.write_text(f"{cores}\t{wall_clock:.3f}\n")
        print(f"[record_wall_clock] cores={cores}  max_benchmark={wall_clock:.3f}s")

###############################################################################
# 2️  aggregate per-split metrics  summary + core-tagged copy
###############################################################################
rule aggregate_stage1:
    input:
        lambda wc: expand("results/{ds}/stage1/metrics_k{K}.tsv",
            ds=wc.ds,K=SIG_SIZES)
    output:
        summary="results/{ds}/stage1/summary_stage1.tsv",
        tagged=f"results/{{ds}}/stage1/summary_stage1_{RUN_CORES}.tsv"
    conda: ENV_YAML
    script: "../scripts/python/aggregate_stage1.py"

###############################################################################
# 3⃣  per-K plots  &  metric overview
###############################################################################
rule plot_stage1:
    input:
        metrics="results/{ds}/stage1/metrics_k{K}.tsv",
        freq="results/{ds}/stage1/freq_k{K}.csv"
    output: directory("figures/{ds}/stage1_k{K}")
    params: title=lambda wc: f"{wc.ds} | K={wc.K}"
    conda: ENV_YAML
    script: "../scripts/python/plots/plot_stage1.py"

rule plot_stage1_summary:
    input: "results/{ds}/stage1/summary_stage1.tsv"
    output:
        MCE="figures/{ds}/stage1_summary/MCE.png",
        Sensitivity="figures/{ds}/stage1_summary/Sensitivity.png",
        Specificity="figures/{ds}/stage1_summary/Specificity.png"
    params: title=lambda wc: wc.ds
    conda: ENV_YAML
    script: "../scripts/python/plots/plot_stage1_summary.py"

###############################################################################
# Stage 2 – fixed-signature evaluation
###############################################################################
rule stage2_fixed_signature:
    input:
        matrix="data/processed/{ds}_matrix.csv",
        gene_set=lambda wc: f"results/{wc.ds}/stage1/freq_k{FIXED_K[wc.ds]}.csv"
    output: "results/{ds}/stage2/metrics.tsv"
    params: fixed=lambda wc: FIXED_K[wc.ds]
    conda: ENV_YAML
    script: "../scripts/python/stage2_eval.py"

###############################################################################
# Phase B  merge timing files + extended runtime plots
###############################################################################
rule merge_wall_clocks:
    """
    Summarize the total wall_clock_{cores}.tsv files
    (one per run_cores) into runtime_by_cores.tsv
    """
    input:
        lambda wc: glob.glob(f"results/{wc.ds}/stage1/wall_clock_*.tsv")
    output:
        "results/{ds}/stage1/runtime_by_cores.tsv"
    run:
        import pandas as pd

        rows = []
        for fp in input:
            with open(fp) as fh:
                line = fh.readline().strip()
                if not line:
                    continue
                core_str, val_str = line.split("\t")
                rows.append((int(core_str), float(val_str)))
        df = pd.DataFrame(rows,columns=["cores", "wall_clock_s"])
        df = df.drop_duplicates("cores",keep="last").sort_values("cores")
        df.to_csv(output[0],sep="\t",index=False)
        print(f"[merge_wall_clocks] wrote {output[0]} ({len(df)} rows)")

rule merge_model_runtimes:
    """
    Summarize summary_stage1_<cores>.tsv across all core runs
    → runtime_by_cores_per_model.tsv
    """
    input:
        lambda wc: glob.glob(f"results/{wc.ds}/stage1/summary_stage1_*.tsv")
    output:
        "results/{ds}/stage1/runtime_by_cores_per_model.tsv"
    run:
        import pandas as pd, re

        frames = []
        for fp in input:
            m = re.search(r"summary_stage1_(\d+|NA)\.tsv$",fp)
            if m:
                cval = m.group(1)
                if cval.isdigit():
                    cores = int(cval)
                else:
                    # if "NA", store as -1 or skip
                    cores = -1
            else:
                cores = -1
            df = pd.read_csv(fp,sep="\t")
            df["cores"] = cores
            frames.append(df)
        outdf = pd.concat(frames,ignore_index=True)
        outdf.to_csv(output[0],sep="\t",index=False)
        print(f"[merge_model_runtimes] wrote {output[0]}")

###############################################################################
# rule plot_runtime
###############################################################################
rule plot_runtime:
    input:
        stage1_summary = "results/{ds}/stage1/summary_stage1.tsv",
        cores_table    = "results/{ds}/stage1/runtime_by_cores.tsv",
        model_table    = "results/{ds}/stage1/runtime_by_cores_per_model.tsv"
    output:
        Train_mean       = "figures/{ds}/runtime/Train_mean.png",
        Pred_mean        = "figures/{ds}/runtime/Pred_mean.png",
        Train_total      = "figures/{ds}/runtime/Train_total.png",
        Pred_total       = "figures/{ds}/runtime/Pred_total.png",
        Runtime_total    = "figures/{ds}/runtime/Runtime_total.png",
        Speed_up         = "figures/{ds}/runtime/Speed_up.png",
        Wall_clock_vs_cores         = "figures/{ds}/runtime/Wall_clock_vs_cores.png",
        Train_total_vs_cores        = "figures/{ds}/runtime/Train_total_vs_cores.png",
        Pred_total_vs_cores         = "figures/{ds}/runtime/Pred_total_vs_cores.png",
        Train_total_vs_cores_zoom   = "figures/{ds}/runtime/Train_total_vs_cores_zoom.png",
        Train_total_vs_cores_fixedK = "figures/{ds}/runtime/Train_total_vs_cores_fixedK.png"
    params:
        title   = lambda wc: wc.ds,
        fixed_k = lambda wc: FIXED_K[wc.ds]
    conda: ENV_YAML
    script: "../scripts/python/plots/plot_runtime.py"
###############################################################################
# Convenience wrapper – host side
###############################################################################
rule aggregate_runtime_all:
    """
    Just a convenience target for host:
      merges wall-clock TSVs, merges per-model summaries,
      and does final runtime plots
    """
    input:
        expand("results/{ds}/stage1/runtime_by_cores.tsv",ds=DATASETS),
        expand("figures/{ds}/runtime/Speed_up.png",ds=DATASETS)



##############################################################################
# Host‑side aggregation
###############################################################################

rule host_analysis:
    output: touch("host_analysis.complete")
    run:
        import pandas as pd, pathlib, glob, re, subprocess

        for ds in DATASETS:
            wc_files = glob.glob(f"results/{ds}/stage1/wall_clock_*.tsv")
            if not wc_files:
                raise ValueError(f"[host_analysis] no wall_clock for {ds}")

            df_wc = (pd.concat([pd.read_csv(f, sep="\t",
                                            names=["cores", "wall_clock_s"])
                                for f in wc_files])
                       .drop_duplicates("cores", keep="last")
                       .sort_values("cores"))
            out_wc = f"results/{ds}/stage1/runtime_by_cores.tsv"
            pathlib.Path(out_wc).parent.mkdir(parents=True, exist_ok=True)
            df_wc.to_csv(out_wc, sep="\t", index=False)

            sum_files = glob.glob(f"results/{ds}/stage1/summary_stage1_*.tsv")
            frames = []
            for fp in sum_files:
                m = re.search(r"summary_stage1_(\d+|NA)\.tsv$", fp)
                cores = int(m.group(1)) if m and m.group(1).isdigit() else -1
                frames.append(pd.read_csv(fp, sep="\t").assign(cores=cores))
            out_mod = f"results/{ds}/stage1/runtime_by_cores_per_model.tsv"
            pd.concat(frames, ignore_index=True).to_csv(out_mod,
                                                        sep="\t", index=False)

        # ---------- render the runtime plots
        subprocess.check_call([
            "snakemake", "-s", "workflow/Snakefile", "--cores", "1",
            *[f"figures/{ds}/runtime/Speed_up.png" for ds in DATASETS],
            "--config", "run_cores=NA",
            "--use-conda",
            "--rerun-triggers", "mtime"
        ])

